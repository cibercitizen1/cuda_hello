// -*- mode: c++ -*-
// -------------------------------------------------------------------
// mainGeneral.cu
// -------------------------------------------------------------------

#include <stdio.h>
#include <cuda.h>


// #include <typeinfo>

//#include <cuda_profiler_api.h>

//#include <cuda_runtime.h>
// #include <helper_cuda.h>
//#include <helper_functions.h>

// -------------------------------------------------------------------
// -------------------------------------------------------------------

// The number of threads per block is influenced by how many
// local memory a kernel uses. The more memory used, the lesser
// number of threads a block can have.
//
// We define the block's number of threads in 2D for we will be
// operate on 2D data.

const unsigned int BLOCK_SIDE = 8;
dim3 THREADS_PER_BLOCK( BLOCK_SIDE, BLOCK_SIDE );

// -------------------------------------------------------------------
// -------------------------------------------------------------------
// As long as  we are using generated data (i.e. not read from a file),
// we choose its size here.
const unsigned int WIDTH_COLUMNS_X = 64; //512;
const unsigned int HEIGHT_ROWS_Y = 64; //512;

// type of the elements on the 2D input data
typedef float Element_Type;

// type of the results
typedef float Result_Type;

// -------------------------------------------------------------------
// -------------------------------------------------------------------
// typedef a name for the texture where we'll put the input data
//
using Texture_Type = texture< Element_Type, 2, cudaReadModeElementType >;


//
// Declared as a global variable
//
Texture_Type ref_data_texture;

// -------------------------------------------------------------------
// -------------------------------------------------------------------
// Declare a reference to a cuda texture (a type of cuda memory,
// allegedly faster, addressed in 2D).
// We will put the input data for the kernels there.
// texture< Element_Type, 2, cudaReadModeElementType > ref_data_texture;

// -------------------------------------------------------------------
// utility for malloc()
// -------------------------------------------------------------------
class Malloc_Error {};

// -------------------------------------------------------------------
template<typename T>
T my_malloc( const long unsigned size )
// spec. no longer needed: throw ( Malloc_Error )
{
  void * ptr = malloc( size );
  if ( ptr == nullptr ) {
	throw Malloc_Error {};
  }
  return static_cast<T>( ptr );
} // ()

// -------------------------------------------------------------------
template<typename T, unsigned int NUM_ROWS, unsigned int NUM_COLUMNS>
auto my_malloc_2D_OK( ) {
  auto ptr = new T[NUM_ROWS][NUM_COLUMNS];
  if ( ptr == nullptr ) {
	throw Malloc_Error {};
  }
  return ptr;
} // ()

// -------------------------------------------------------------------
// Let's use this versiono with cudaMallocHost()
// with gets "pinned memory" in the CPU for us.
// I guess that means that the memory is aligned so that transfers
// from and to the GPU are faster.
template<typename T, unsigned int NUM_ROWS, unsigned int NUM_COLUMNS>
auto my_malloc_2D( ) {
  T* ptr = nullptr;
  size_t size = NUM_ROWS * NUM_COLUMNS * sizeof( T );
  // printf( "my_malloc_2D(): size=%zu\n", size );
  cudaMallocHost( & ptr, size );
  if ( ptr == nullptr ) {
	throw Malloc_Error {};
  }
  //return static_cast< T (*)[NUM_COLUMNS] >( ptr );
  return ( T (*)[NUM_COLUMNS] ) ptr;
} // ()

/*
  //auto kk = new int [10][20];
  // OK int (* kk)[20] = new int [10][20];
  int (* kk)[20] = new int [10][20];
  kk[9][2] = 13;
*/

// -------------------------------------------------------------------
//
// kernel
//
// -------------------------------------------------------------------
__global__ void test_kernel_1( Result_Type * p_results,
							   unsigned int width,
							   unsigned int height ) {
  //			   Texture_Type input_texture ) {

  unsigned int x_column = (blockIdx.x * blockDim.x) + threadIdx.x;
  unsigned int y_row = (blockIdx.y * blockDim.y) + threadIdx.y;

  Element_Type val = tex2D( ref_data_texture, x_column+0.5f, y_row+0.5f );

  p_results[ (width * y_row) + x_column ] = -val;
	
} // ()
							   

// -------------------------------------------------------------------
//
// main
//
// -------------------------------------------------------------------
int main( int n_args, char * args[] ) {

  printf( " starting \n" );

  // .................................................................
  //
  // Create the input data for the kernels
  // to compute something on it
  //
  // .................................................................
  
  //
  // malloc on the host
  //

  /*
	get memory in the traditional way: an array of bytes
	This way, we must do the maths to access 2D data
	
  Element_Type* p_data2 = my_malloc< Element_Type* >( WIDTH_COLUMNS_X * HEIGHT_ROWS_Y * sizeof( Element_Type ) );

	  //p_data2[ row*WIDTH_COLUMNS_X + col ] = row + col/1000.0;
  */

  /*auto* p_data = */
  Element_Type (*p_data)[WIDTH_COLUMNS_X] =
	my_malloc_2D< Element_Type, HEIGHT_ROWS_Y, WIDTH_COLUMNS_X>();
  // Note: p_data[ rows = HEIGHT_ROWS_Y ][ columns = WIDTH_COLUMNS_X ]

  printf( " got the memory for input data \n" );

  //
  // fill in the data
  //
  // each element is a float row.col. Ex. 10.15 is row 10, col 15
  for ( unsigned int row = 0; row < HEIGHT_ROWS_Y; row++ ) {
	//printf( " row %d \n", row );
	for ( unsigned int col = 0; col < WIDTH_COLUMNS_X; col++ ) {
	  p_data[ row ][ col ] = row + col/1000.0;
	} // for
  } // for

  //printf( " %f \n", p_data2[ 10*WIDTH_COLUMNS_X + 15 ] );
  printf( " %f \n", p_data[10][15] );

  // .................................................................
  // Put the data on the device, in a texture
  // .................................................................
  // Declare a reference to a cuda texture (a type of cuda memory,
  // allegedly faster, addressed in 2D).
  // We will put the input data for the kernels there.
  //
  // Declared as a global variable Texture_Type ref_data_texture;
  // OK texture< Element_Type, 2, cudaReadModeElementType > ref_data_texture;

  //
  // create a 2D array on the GPU to place our data
  //
  cudaArray_t cuda_array_for_our_data;

  /* do we need this?
  cudaChannelFormatDesc channel_desc =
	cudaCreateChannelDesc( 32, 0, 0, 0, cudaChannelFormatKindFloat );
  */

  // cudaMallocArray( &data_cuda_array, &channel_desc, 32, 32 );
  cudaMallocArray( &cuda_array_for_our_data, &ref_data_texture.channelDesc, 
				   WIDTH_COLUMNS_X, // number of rows
				   HEIGHT_ROWS_Y // number of columns
				   );


	//  sizeof( Element_Type ) );
	/* size of a row:
	 * number of columns
	 * times
	 * size of element */
	/*
	  cudaMallocArray( &data_cuda_array,
	  &data_cuda_array.channelDesc, 
	  WIDTH_COLUMNS_X, // number of rows
	  HEIGHT_ROWS_Y * sizeof( Element_Type ) );*/ /* size of a row:
											* number of columns
											* times
											* size of element */

  //
  // copy the data to the array on the device
  //
  // Note: we consulted Cuda C++ Prgramming Guide v12.0 page 74
  cudaMemcpy2DToArray( cuda_array_for_our_data,
					   0, 0,
					   p_data,
					   WIDTH_COLUMNS_X * sizeof( Element_Type ),
					   WIDTH_COLUMNS_X * sizeof( Element_Type ),
					   HEIGHT_ROWS_Y,
					   cudaMemcpyHostToDevice );
  //
  // bind texture with array
  //
  cudaBindTextureToArray( ref_data_texture, cuda_array_for_our_data );
					   
  // .................................................................
  // Get memory to hold the results
  // Let's suppose that we get a result for each input element.
  // .................................................................
  //
  // Get memory on the host.
  //
  Result_Type (*p_results)[WIDTH_COLUMNS_X] =
	my_malloc_2D< Result_Type, HEIGHT_ROWS_Y, WIDTH_COLUMNS_X>();
  
  //
  // Get memory on the device. Regular memory I guess, i.e. not a texture.
  //
  // Right now: I don't the differences between cudaMalloc and
  // cudaMallocManaged.

  Result_Type * p_results_on_device;
  cudaMallocManaged( &p_results_on_device,
					 HEIGHT_ROWS_Y * WIDTH_COLUMNS_X * sizeof( Result_Type )
					 );
				   
  // .................................................................
  // set up the launch of kernels
  // .................................................................
  
  //
  // Number of blocks we need considering a thread per element (pixel)
  // in the 2D data
  // Defined in 2D.
  dim3 NUM_BLOCKS( WIDTH_COLUMNS_X / THREADS_PER_BLOCK.x,
				   HEIGHT_ROWS_Y / THREADS_PER_BLOCK.y );

  // .................................................................
  // Launch the kernel
  // .................................................................
  test_kernel_1<<< NUM_BLOCKS, THREADS_PER_BLOCK >>>( p_results_on_device, 
													 WIDTH_COLUMNS_X,
													 HEIGHT_ROWS_Y );

  // .................................................................
  // Copy results from memory device
  // .................................................................
  cudaMemcpy( p_results,
			  p_results_on_device, 
			  HEIGHT_ROWS_Y * WIDTH_COLUMNS_X * sizeof( Result_Type ),
			  cudaMemcpyDeviceToHost );

  // show sth. to check if the kernel has done something
  printf( " %f \n", p_results[10][15] );
  printf( " %f \n", p_results[32][40] );
  
  // .................................................................
  // .................................................................
	printf( " all done \n" );
} // ()

// -------------------------------------------------------------------
// -------------------------------------------------------------------
// -------------------------------------------------------------------
// -------------------------------------------------------------------
