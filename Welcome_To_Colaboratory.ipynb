{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cibercitizen1/cuda_hello/blob/main/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liK2l-IVWIcR",
        "outputId": "0c932764-9f44-4b14-908f-7059dffee038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxYOiK5mWOAh",
        "outputId": "f764975c-c093-4ddc-82f6-add25f5db96a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-2wsfbbfz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-2wsfbbfz\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4304 sha256=04a1bb26eabe76c397586d2b66e1eb22c56e96f459a62806569f3f444cd4574e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q555m6na/wheels/f3/08/cc/e2b5b0e1c92df07dbb50a6f024a68ce090f5e7b2316b41756d\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEV5vdolWTXh",
        "outputId": "bb27a91b-9a4b-4e73-ccea-74c11879215f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "// -------------------------------------------------------------\n",
        "// mainHello_1.cu\n",
        "// -------------------------------------------------------------\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        "\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "using namespace std;\n",
        "\n",
        "// -------------------------------------------------------------\n",
        "// Z -> () -> Z (for a kernel)\n",
        "//\n",
        "// [Z] -> () -> [Z] (for all the kernels)\n",
        "// -------------------------------------------------------------\n",
        "__global__ void test_kernel(int* p_input, int* p_output) {\n",
        "\n",
        "  //\n",
        "  // We wave 1-dim data (i.e. an array)\n",
        "  // We have arranged one thread for one cell\n",
        "  // both in the input array and in the output one\n",
        "  // \n",
        "  // Therefore, we have to find out our thread index,\n",
        "  // which equates to the cell number in the array\n",
        "  // we have to manipulate\n",
        "  //\n",
        "  // The calculation is as follows:\n",
        "  \n",
        "  int idx = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "\n",
        "  // blockDim.x: the number of threads in the block for the x index\n",
        "  // (which in this case is the only one)\n",
        "  // times\n",
        "  // blockIdx.x: number of block in for the x index\n",
        "  // plus\n",
        "  // threadIdx.x: the thread number within this block\n",
        "  // Example: if we are the block number 3, each block has 16 threds\n",
        "  // and the thread number is 7\n",
        "  // The cell would be 3*16 + 7\n",
        "\n",
        "  //\n",
        "  // This is the calculation\n",
        "  //\n",
        "  p_output[idx] =  100 + p_input[idx];\n",
        "\n",
        "}\n",
        "\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "int main() {\n",
        "\n",
        "  //\n",
        "  // input and output local arrays\n",
        "  //\n",
        "  const int N=1024;\n",
        "  int numbers[N];\n",
        "  int results[N];\n",
        "\n",
        "\t int tam = N * sizeof(int);\n",
        "\n",
        "  for (int i = 0; i <= N-1; i++) {\n",
        "\tnumbers[i] = i;\n",
        "\tresults[i] = -1;\n",
        "  }\n",
        "\n",
        " \n",
        " \n",
        "\n",
        "  //\n",
        "  // get memory in the device\n",
        "  //\n",
        "  int* p_in;\n",
        "  int* p_out;\n",
        "  //\n",
        "  cudaMalloc(&p_in, tam);\n",
        "  cudaMalloc(&p_out, tam);\n",
        "\n",
        "  //\n",
        "  // timers, define and start to count\n",
        "  //\n",
        "  cudaEvent_t start; \n",
        "  cudaEvent_t end;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&end);\n",
        "  \n",
        "  cudaEventRecord(start);\n",
        "\n",
        "  //\n",
        "  // copy to device\n",
        "  //\n",
        "  cudaMemcpy(p_in, numbers, tam, cudaMemcpyHostToDevice);\n",
        "\n",
        "  dim3 total_blocks( 4 );\n",
        "  dim3 threads_per_block( N/4 );\n",
        "  \n",
        "  // dim3 total_blocks( 1 );\n",
        "  // dim3 threads_per_block( N );\n",
        "\n",
        "\n",
        "  //\n",
        "  // start up the kernel(s)\n",
        "  //\n",
        "  test_kernel<<<total_blocks, threads_per_block>>>(p_in, p_out);\n",
        "\n",
        "  //\n",
        "  // wait for completion\n",
        "  //\n",
        "  cudaEventSynchronize(end);\n",
        "\n",
        "  //\n",
        "  // copy from device\n",
        "  //\n",
        "  cudaMemcpy(&results[0], p_out,  tam, cudaMemcpyDeviceToHost);\n",
        "  \n",
        "  //\n",
        "  // record end moment, and calculate the elapsed time\n",
        "  //\n",
        "  cudaEventRecord(end);\n",
        "  float time = 0;\n",
        "  cudaEventElapsedTime(&time, start, end);\n",
        "\n",
        "  //\n",
        "  // results\n",
        "  //\n",
        "\n",
        "  cout << \"results[1] : \" << results[1] << endl;\n",
        "\n",
        "  cout << \"results[31] : \" << results[31] << endl;\n",
        "  cout << \"results[32] : \" << results[32] << endl;\n",
        "\n",
        "  cout << \"results[63] : \" << results[63] << endl;\n",
        "  cout << \"results[64] : \" << results[64] << endl;\n",
        "  cout << \"results[65] : \" << results[65] << endl;\n",
        "  cout << \"results[\" << N-1 << \"] : \" << results[N-1] << endl;\n",
        "\n",
        "\n",
        "  cout << \"start: \" << start << endl;\n",
        "  cout << \"end: \" << end << endl;\n",
        "  //cout << (end - start) << endl;\n",
        "  cout << \"The time required : \";\n",
        "  cout << time << endl;\n",
        "} // main()\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v_OM1eJehzs",
        "outputId": "f458c6c9-368b-4dd4-afa3-774efe4294a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results[1] : 101\n",
            "results[31] : 131\n",
            "results[32] : 132\n",
            "results[63] : 163\n",
            "results[64] : 164\n",
            "results[65] : 165\n",
            "results[1023] : 1123\n",
            "start: 0x56114121de10\n",
            "end: 0x56114121dc70\n",
            "The time required : 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqYHXApU9Us7",
        "outputId": "8f491624-97db-4cfd-dd74-a80168f88a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oths9WePyIr5",
        "outputId": "3c4198bc-3a6d-4aea-b45e-4c45e3dfd665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "/* This sample queries the properties of the CUDA devices present in the system via CUDA Runtime API. */\n",
        "\n",
        "// includes, system\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "\n",
        "// includes, project\n",
        "// #include <cutil.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "int\n",
        "main( int argc, char** argv) \n",
        "{\n",
        "    printf(\"CUDA Device Query (Runtime API) version (CUDART static linking)\\n\");\n",
        "\n",
        "    int deviceCount = 0;\n",
        "\n",
        "\tif (cudaGetDeviceCount(&deviceCount) != cudaSuccess) {\n",
        "\t\tprintf(\"cudaGetDeviceCount failed! CUDA Driver and Runtime version may be mismatched.\\n\");\n",
        "\t\tprintf(\"\\nTest FAILED!\\n\");\n",
        "\t\treturn 0;\n",
        "\t}\n",
        "\n",
        "    // This function call returns 0 if there are no CUDA capable devices.\n",
        "    if (deviceCount == 0)\n",
        "        printf(\"There is no device supporting CUDA\\n\");\n",
        "\n",
        "    int dev;\n",
        "    for (dev = 0; dev < deviceCount; ++dev) {\n",
        "        cudaDeviceProp deviceProp;\n",
        "        cudaGetDeviceProperties(&deviceProp, dev);\n",
        "\n",
        "        if (dev == 0) {\n",
        "\t\t\t// This function call returns 9999 for both major & minor fields, if no CUDA capable devices are present\n",
        "            if (deviceProp.major == 9999 && deviceProp.minor == 9999)\n",
        "                printf(\"There is no device supporting CUDA.\\n\");\n",
        "            else if (deviceCount == 1)\n",
        "                printf(\"There is 1 device supporting CUDA\\n\");\n",
        "            else\n",
        "                printf(\"There are %d devices supporting CUDA\\n\", deviceCount);\n",
        "        }\n",
        "        printf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
        "    #if CUDART_VERSION >= 2020\n",
        "\t\tint driverVersion = 0, runtimeVersion = 0;\n",
        "\t\tcudaDriverGetVersion(&driverVersion);\n",
        "\t\tprintf(\"  CUDA Driver Version:                           %d.%d\\n\", driverVersion/1000, driverVersion%100);\n",
        "\t\tcudaRuntimeGetVersion(&runtimeVersion);\n",
        "\t\tprintf(\"  CUDA Runtime Version:                          %d.%d\\n\", runtimeVersion/1000, runtimeVersion%100);\n",
        "    #endif\n",
        "\n",
        "        printf(\"  CUDA Capability Major revision number:         %d\\n\", deviceProp.major);\n",
        "        printf(\"  CUDA Capability Minor revision number:         %d\\n\", deviceProp.minor);\n",
        "\n",
        "\t\tprintf(\"  Total amount of global memory:                 %u bytes\\n\", deviceProp.totalGlobalMem);\n",
        "    #if CUDART_VERSION >= 2000\n",
        "        printf(\"  Number of multiprocessors:                     %d\\n\", deviceProp.multiProcessorCount);\n",
        "        printf(\"  Number of cores:                               %d\\n\", 8 * deviceProp.multiProcessorCount);\n",
        "    #endif\n",
        "        printf(\"  Total amount of constant memory:               %u bytes\\n\", deviceProp.totalConstMem); \n",
        "        printf(\"  Total amount of shared memory per block:       %u bytes\\n\", deviceProp.sharedMemPerBlock);\n",
        "        printf(\"  Total number of registers available per block: %d\\n\", deviceProp.regsPerBlock);\n",
        "        printf(\"  Warp size:                                     %d\\n\", deviceProp.warpSize);\n",
        "        printf(\"  Maximum number of threads per block:           %d\\n\", deviceProp.maxThreadsPerBlock);\n",
        "        printf(\"  Maximum sizes of each dimension of a block:    %d x %d x %d\\n\",\n",
        "               deviceProp.maxThreadsDim[0],\n",
        "               deviceProp.maxThreadsDim[1],\n",
        "               deviceProp.maxThreadsDim[2]);\n",
        "        printf(\"  Maximum sizes of each dimension of a grid:     %d x %d x %d\\n\",\n",
        "               deviceProp.maxGridSize[0],\n",
        "               deviceProp.maxGridSize[1],\n",
        "               deviceProp.maxGridSize[2]);\n",
        "        printf(\"  Maximum memory pitch:                          %u bytes\\n\", deviceProp.memPitch);\n",
        "        printf(\"  Texture alignment:                             %u bytes\\n\", deviceProp.textureAlignment);\n",
        "        printf(\"  Clock rate:                                    %.2f GHz\\n\", deviceProp.clockRate * 1e-6f);\n",
        "    #if CUDART_VERSION >= 2000\n",
        "        printf(\"  Concurrent copy and execution:                 %s\\n\", deviceProp.deviceOverlap ? \"Yes\" : \"No\");\n",
        "    #endif\n",
        "    #if CUDART_VERSION >= 2020\n",
        "        printf(\"  Run time limit on kernels:                     %s\\n\", deviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n",
        "        printf(\"  Integrated:                                    %s\\n\", deviceProp.integrated ? \"Yes\" : \"No\");\n",
        "        printf(\"  Support host page-locked memory mapping:       %s\\n\", deviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n",
        "        printf(\"  Compute mode:                                  %s\\n\", deviceProp.computeMode == cudaComputeModeDefault ?\n",
        "\t\t\t                                                            \"Default (multiple host threads can use this device simultaneously)\" :\n",
        "\t\t                                                                deviceProp.computeMode == cudaComputeModeExclusive ?\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Exclusive (only one host thread at a time can use this device)\" :\n",
        "\t\t                                                                deviceProp.computeMode == cudaComputeModeProhibited ?\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Prohibited (no host thread can use this device)\" :\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Unknown\");\n",
        "    #endif\n",
        "\t}\n",
        "    printf(\"\\nTest PASSED\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAvTzcmD854H",
        "outputId": "fdfa274d-ec14-4c96-8f6a-f3406e00821d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "There is 1 device supporting CUDA\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version:                           11.60\n",
            "  CUDA Runtime Version:                          11.20\n",
            "  CUDA Capability Major revision number:         7\n",
            "  CUDA Capability Minor revision number:         5\n",
            "  Total amount of global memory:                 2958950400 bytes\n",
            "  Number of multiprocessors:                     40\n",
            "  Number of cores:                               320\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per block:           1024\n",
            "  Maximum sizes of each dimension of a block:    1024 x 1024 x 64\n",
            "  Maximum sizes of each dimension of a grid:     2147483647 x 65535 x 65535\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Clock rate:                                    1.59 GHz\n",
            "  Concurrent copy and execution:                 Yes\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated:                                    No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Compute mode:                                  Default (multiple host threads can use this device simultaneously)\n",
            "\n",
            "Test PASSED\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "// -------------------------------------------------------------\n",
        "// mainHello_2.cu\n",
        "// -------------------------------------------------------------\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        "\n",
        "\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "using namespace std;\n",
        "\n",
        "// -------------------------------------------------------------\n",
        "// Shift to the left the contents of the array \n",
        "// -------------------------------------------------------------\n",
        "__global__ void test_kernel( int* p_data, const int N ) {\n",
        "\n",
        "  int aux;\n",
        "\n",
        "  // find out my id\n",
        "  int idx = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "\n",
        "  // let's slow down some threads\n",
        "  if ( idx % 3 == 0) {\n",
        "    // read \"my own\" element\n",
        "    aux = p_data[idx]; \n",
        "\n",
        "    // just to keep the thread busy\n",
        "    // and have the [idx] element with\n",
        "    // a wrong value during this lapse\n",
        "    for (int i=1; i<=10*10*10*10; i++ ) {\n",
        "      p_data[idx] = -1234;\n",
        "      p_data[idx] = floorf( sinf( i*i ) );\n",
        "    } // for\n",
        "\n",
        "    // finaly we put back the original value of [idx]\n",
        "    p_data[idx] = aux; \n",
        "  } // if\n",
        "\n",
        "  // supposedly, all threads stop here\n",
        "  // so that when they pass this point\n",
        "  // the array is as before\n",
        "  __syncthreads(); \n",
        "  \n",
        "  // caution: __syncthreads only syncs threads\n",
        "  // corresponding to the same block\n",
        "  // Thus, if there are several blocks one\n",
        "  // thread per block migh read a wrong value\n",
        "  // (i.e. not the original element but\n",
        "  // the new one if already set by its neighbour)\n",
        "  \n",
        "  //\n",
        "  // shift to the left in two steps: 1 read 2 write\n",
        "  //\n",
        "  // 1 read\n",
        "  aux = p_data[ (idx+1) % N ];\n",
        "  \n",
        "  //\n",
        "  // each thread should wait until the rest \n",
        "  // have read the value of its neighbour element.\n",
        "  //\n",
        "  __syncthreads(); \n",
        "\n",
        "  // 2 write\n",
        "  p_data[idx] = aux;\n",
        "\n",
        "} // ()\n",
        "\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "int main() {\n",
        "\n",
        "  //\n",
        "  // input and output local arrays\n",
        "  //\n",
        "  const int N=1024;\n",
        "  int numbers[N];\n",
        "\n",
        "\t int tam = N * sizeof(int);\n",
        "\n",
        "  for (int i = 0; i <= N-1; i++) {\n",
        "\t  numbers[i] = i;\n",
        "  }\n",
        "\n",
        "  //\n",
        "  // get memory in the device\n",
        "  //\n",
        "  int* p_data;\n",
        "  //\n",
        "  cudaMalloc(&p_data, tam);\n",
        "\n",
        "  //\n",
        "  // copy to device\n",
        "  //\n",
        "  cudaMemcpy(p_data, numbers, tam, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // we want 1 thread per element => N threads\n",
        "  const int NUM_BLOCKS = 8;\n",
        "  dim3 total_blocks( NUM_BLOCKS );\n",
        "  dim3 threads_per_block( N / NUM_BLOCKS );\n",
        "  \n",
        "  //\n",
        "  // timer\n",
        "  //\n",
        "  cudaEvent_t end;\n",
        "  cudaEventCreate(&end);\n",
        "\n",
        "  //\n",
        "  // start up the kernel(s)\n",
        "  //\n",
        "  test_kernel<<<total_blocks, threads_per_block>>>(p_data, N);\n",
        "\n",
        "  //\n",
        "  // wait for completion\n",
        "  //\n",
        "  cudaEventSynchronize(end);\n",
        "\n",
        "  //\n",
        "  // copy from device\n",
        "  //\n",
        "  cudaMemcpy(&numbers[0], p_data,  tam, cudaMemcpyDeviceToHost);\n",
        "  \n",
        "  //\n",
        "  // results\n",
        "  //\n",
        "\n",
        "  cout << \"numbers[0] : \" << numbers[0] << endl;\n",
        "  cout << \"numbers[1] : \" << numbers[1] << endl;\n",
        "\n",
        "  cout << \"numbers[12] : \" << numbers[12] << endl;\n",
        "  cout << \"numbers[13] : \" << numbers[13] << endl;\n",
        "  cout << \"numbers[14] : \" << numbers[14] << endl;\n",
        "\n",
        "  cout << \"numbers[31] : \" << numbers[31] << endl;\n",
        "  cout << \"numbers[32] : \" << numbers[32] << endl;\n",
        "  cout << \"numbers[33] : \" << numbers[33] << endl;\n",
        "\n",
        "  cout << \"numbers[\" << N-1 << \"] : \" << numbers[N-1] << endl;\n",
        "\n",
        "  for (int i=0; i<=N-1; i++) {\n",
        "     if (numbers[i] != (i+1) % N ) {\n",
        "      cout << \"element \" << i << \" is wrong\" << endl;\n",
        "      cout << \"numbers[\" << i << \"] : \" << numbers[i] << endl;\n",
        "     }\n",
        "  }\n",
        "\n",
        "} // main()\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP2fSnrSFfEp",
        "outputId": "9323b0a0-4cce-4181-9faa-963202b38d4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numbers[0] : 1\n",
            "numbers[1] : 2\n",
            "numbers[12] : 13\n",
            "numbers[13] : 14\n",
            "numbers[14] : 15\n",
            "numbers[31] : 32\n",
            "numbers[32] : 33\n",
            "numbers[33] : 34\n",
            "numbers[1023] : 0\n",
            "element 127 is wrong\n",
            "numbers[127] : 129\n",
            "element 255 is wrong\n",
            "numbers[255] : 257\n",
            "element 383 is wrong\n",
            "numbers[383] : -1234\n",
            "element 511 is wrong\n",
            "numbers[511] : 513\n",
            "element 639 is wrong\n",
            "numbers[639] : 641\n",
            "element 767 is wrong\n",
            "numbers[767] : -1234\n",
            "element 895 is wrong\n",
            "numbers[895] : 897\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "// -*- mode: c++ -*-\n",
        "// ===================================================================\n",
        "// mainGeneral.cu\n",
        "// ===================================================================\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "//#include <cuda.h>\n",
        "\n",
        "// #include <typeinfo>\n",
        "\n",
        "// ===================================================================\n",
        "// ===================================================================\n",
        "\n",
        "// The number of threads per block is influenced by how many\n",
        "// local memory a kernel uses. The more memory used, the lesser\n",
        "// number of threads a block can have.\n",
        "//\n",
        "// We define the block's number of threads in 2D for we will be\n",
        "// operate on 2D data.\n",
        "\n",
        "const unsigned int BLOCK_SIDE = 8;\n",
        "dim3 THREADS_PER_BLOCK( BLOCK_SIDE, BLOCK_SIDE );\n",
        "\n",
        "// ===================================================================\n",
        "// ===================================================================\n",
        "// As long as  we are using generated data (i.e. not read from a file),\n",
        "// we choose its size here.\n",
        "const unsigned int WIDTH_COLUMNS_X = 128; //512;\n",
        "const unsigned int HEIGHT_ROWS_Y = 128; //512;\n",
        "\n",
        "// type of the elements on the 2D input data\n",
        "typedef float Element_Type;\n",
        "\n",
        "// type of the results\n",
        "typedef float Result_Type;\n",
        "\n",
        "// ===================================================================\n",
        "// ===================================================================\n",
        "void check_cuda_call( const char * msg = \"\" ) {\n",
        "  \n",
        "  cudaError_t error = cudaGetLastError();\n",
        "  \n",
        "  if ( error != cudaSuccess ) {\n",
        "      \n",
        "   printf( \" check_cuda_call: failed %s, reason: %s \\n\", msg, cudaGetErrorString( error ));\n",
        "   exit(0);\n",
        "    assert( error == cudaSuccess );\n",
        "  } \n",
        "} // ()\n",
        "\n",
        "// ===================================================================\n",
        "// utility for malloc()\n",
        "// ===================================================================\n",
        "//class Malloc_Error {};\n",
        "\n",
        "// ===================================================================\n",
        "template<typename T>\n",
        "T my_malloc( const long unsigned size )\n",
        "// spec. no longer needed: throw ( Malloc_Error )\n",
        "{\n",
        "  void * ptr = malloc( size );\n",
        "\n",
        "  assert( ptr != nullptr && \"my_malloc failed\" );\n",
        "\n",
        "  return static_cast<T>( ptr );\n",
        "} // ()\n",
        "\n",
        "// ===================================================================\n",
        "/*\n",
        "template<typename T, unsigned int NUM_ROWS, unsigned int NUM_COLUMNS>\n",
        "auto my_malloc_2D_OK( ) {\n",
        "  auto ptr = new T[NUM_ROWS][NUM_COLUMNS];\n",
        "  if ( ptr == nullptr ) {\n",
        "\tthrow Malloc_Error {};\n",
        "  }\n",
        "  return ptr;\n",
        "} // ()\n",
        "*/\n",
        "\n",
        "// ===================================================================\n",
        "// Let's use cudaMallocHost()\n",
        "// which gets \"pinned memory\" in the CPU for us.\n",
        "// I guess that means that the memory is aligned so that transfers\n",
        "// from and to the GPU are faster.\n",
        "template<typename T>\n",
        "T * my_malloc_2D( unsigned int NUM_ROWS, unsigned int NUM_COLUMNS) {\n",
        "  \n",
        "  //\n",
        "  // compute the size required\n",
        "  //\n",
        "  size_t size = NUM_ROWS * NUM_COLUMNS * sizeof( T );\n",
        "\n",
        "  //printf( \"my_malloc_2D(): rows=%d columns=%d, size=%d\\n\", NUM_ROWS, NUM_COLUMNS, size );\n",
        "\n",
        "  //\n",
        "  // malloc\n",
        "  //\n",
        "  T* ptr = nullptr;\n",
        "  cudaMallocHost( & ptr, size );\n",
        "\n",
        "  check_cuda_call( \"my_malloc_2D(): cudaMallocHost()\" );\n",
        "  \n",
        "  //\n",
        "  // make sure we've got memory\n",
        "  //\n",
        "  assert( ptr != nullptr && \"my_malloc_2D failed\" );\n",
        "\n",
        "  //return ( T * [] ) ptr;\n",
        "  return ptr;\n",
        "} // ()\n",
        "\n",
        "/*\n",
        "  //auto kk = new int [10][20];\n",
        "  // OK int (* kk)[20] = new int [10][20];\n",
        "  int (* kk)[20] = new int [10][20];\n",
        "  kk[9][2] = 13;\n",
        "*/\n",
        "\n",
        "// ===================================================================\n",
        "// Utility class for allocating memory both on the device\n",
        "// and on the host.\n",
        "// ===================================================================\n",
        "template<typename T>\n",
        "class Results_Holder {\n",
        "private:\n",
        "  const unsigned int NUM_ROWS;\n",
        "  const unsigned int NUM_COLUMNS;\n",
        "public:\n",
        "  T * results_on_host;\n",
        "  T * results_on_device;\n",
        "\n",
        "  // -----------------------------------------------------------------\n",
        "  // Used to access to the correct row of results_on_host.\n",
        "  // Column dimension is required to get the correct one.\n",
        "  // Because a pointer is returned, [] can be chained:\n",
        "  // Example:\n",
        "  // results[10][15]\n",
        "  // -----------------------------------------------------------------\n",
        "  const T & operator()( unsigned int row, unsigned int col ) {\n",
        "\treturn  results_on_host[ (row * NUM_COLUMNS) + col ];\n",
        "  } // ()\n",
        "\n",
        "  // -----------------------------------------------------------------\n",
        "  // destructor\n",
        "  // -----------------------------------------------------------------\n",
        "  ~Results_Holder( ) {\n",
        "\tcudaFree( results_on_host );\n",
        "\tcudaFree( results_on_device );\n",
        "\tprintf( \" results memory (host and device) freed \\n\" );\n",
        "  } // ()\n",
        "\n",
        "  // -----------------------------------------------------------------\n",
        "  // constructor\n",
        "  // -----------------------------------------------------------------\n",
        "  Results_Holder( unsigned int num_rows, unsigned int num_columns )\n",
        "\t: NUM_ROWS( num_rows ), NUM_COLUMNS( num_columns )\n",
        "  {\n",
        "\t//\n",
        "\t// Get memory on the host.\n",
        "\t//\n",
        "\tresults_on_host = my_malloc_2D< T >( NUM_ROWS, NUM_COLUMNS );\n",
        "  \n",
        "\t//\n",
        "\t// Get memory on the device. Regular memory I guess, i.e. not a texture.\n",
        "\t//\n",
        "\t// Right now: I don't the differences between cudaMalloc and\n",
        "\t// cudaMallocManaged.\n",
        "\t//\n",
        "\tcudaMallocManaged( & results_on_device,\n",
        "\t\t\t\t\t   NUM_ROWS * NUM_COLUMNS * sizeof( T )\n",
        "\t\t\t\t\t   );\n",
        "\n",
        "\tcheck_cuda_call( \" Results_Holder: cudaMallocManaged()\" );\n",
        "  } // ()\n",
        "\n",
        "  // -----------------------------------------------------------------\n",
        "  // -----------------------------------------------------------------\n",
        "  void copy_results_device_to_host() {\n",
        "\tcudaMemcpy( results_on_host,\n",
        "\t\t\t\tresults_on_device,\n",
        "\t\t\t\tNUM_ROWS * NUM_COLUMNS * sizeof( T ),\n",
        "\t\t\t\tcudaMemcpyDeviceToHost );\n",
        "\tcheck_cuda_call( \" copy_results_device_to_host \" );\n",
        "  } // ()\n",
        "\n",
        "  // -----------------------------------------------------------------\n",
        "  // -----------------------------------------------------------------\n",
        "}; // class\n",
        "\n",
        "// ===================================================================\n",
        "// Utility class for allocating memory on the device \n",
        "// binding it to a texture and copying the input data on the host\n",
        "// to it.\n",
        "// ===================================================================\n",
        "template<typename T>\n",
        "class Texture_Memory_Holder {\n",
        "private:\n",
        "  const unsigned int NUM_ROWS;\n",
        "  const unsigned int NUM_COLUMNS;\n",
        "public:\n",
        "  \n",
        "  cudaChannelFormatDesc channel_desc;\n",
        "\n",
        "  T* data_on_device;\n",
        "  \n",
        "  cudaTextureObject_t texture;\n",
        "  cudaResourceDesc resource_desc;\n",
        "  cudaTextureDesc texture_desc;\n",
        "\n",
        "  // -----------------------------------------------------------------\n",
        "  // destructor\n",
        "  // -----------------------------------------------------------------\n",
        "  ~Texture_Memory_Holder( ) {\n",
        "\tcudaFree( data_on_device );\n",
        "\tcudaDestroyTextureObject( texture );\n",
        "\tprintf( \" data_on_device and texture memory freed \\n\" );\n",
        "  } // ()\n",
        "\n",
        "  // -----------------------------------------------------------------\n",
        "  // constructor\n",
        "  // -----------------------------------------------------------------\n",
        "  Texture_Memory_Holder(\n",
        "\t\t\t\t\t   Element_Type (*p_data)[],\n",
        "\t\t\t\t\t   unsigned int num_rows,\n",
        "\t\t\t\t\t   unsigned int num_columns\n",
        "\t\t\t\t\t   )\n",
        "\t: NUM_ROWS( num_rows ), NUM_COLUMNS( num_columns )\n",
        "  {\n",
        "\n",
        "\t//\n",
        "\t// get memory on the GPU to place our data\n",
        "\t//\n",
        "\n",
        "\tsize_t total_size = NUM_ROWS * NUM_COLUMNS * sizeof( T ); \n",
        "\n",
        "\tcudaMalloc( & data_on_device, total_size );\n",
        "\n",
        "\tcheck_cuda_call( \" Texture_Memory_Holder: cudaMalloc() \" );\n",
        "\n",
        "  printf( \" Texture_Memory_Holder: element_type_size=%d, rows=%d, cols=%d, total_size=%zu\\n\", \n",
        "        sizeof( Element_Type ),\n",
        "         NUM_ROWS, NUM_COLUMNS, total_size );\n",
        "\n",
        "\t//\n",
        "\t// copy the data from here to the memory on the device\n",
        "\t//\n",
        "\tcudaMemcpy( data_on_device, // destination\n",
        "\t\t\t\t\t\t p_data, // source\n",
        "             total_size, // size\n",
        "\t\t\t\t\t\t cudaMemcpyHostToDevice );\n",
        "\n",
        "  check_cuda_call( \" Texture_Memory_Holder: cudaMemcpy() \" );\n",
        "\n",
        "\t//\n",
        "\t// create a channel.  What is this for?\n",
        "\t//\n",
        "\tchannel_desc =\n",
        "    cudaCreateChannelDesc< Element_Type >();\n",
        "\t  //cudaCreateChannelDesc( 32, 0, 0, 0, cudaChannelFormatKindFloat );\n",
        "\n",
        "\tcheck_cuda_call( \" Texture_Memory_Holder: cudaCreateChannelDesc() \" );\n",
        "\n",
        "\t//\n",
        "\t// create and configure a texture\n",
        "\t//\n",
        "\tmemset( & resource_desc, 0, sizeof( cudaResourceDesc ) );\n",
        "\n",
        "\tresource_desc.resType = cudaResourceTypePitch2D;\n",
        "\n",
        "  resource_desc.res.pitch2D.devPtr = data_on_device;\n",
        "\n",
        "  resource_desc.res.pitch2D.width = NUM_COLUMNS;\n",
        "  resource_desc.res.pitch2D.height = NUM_ROWS;\n",
        "\n",
        "  resource_desc.res.pitch2D.desc = channel_desc;\n",
        "\n",
        "  resource_desc.res.pitch2D.pitchInBytes = NUM_COLUMNS * sizeof( Element_Type );\n",
        "\n",
        "  //\n",
        "  //\n",
        "  //\n",
        "\tmemset( & texture_desc, 0, sizeof( cudaTextureDesc ) );\n",
        "\n",
        "\t// Last time I set this. Why?\n",
        "\t//texture_desc.normalizedCoords = false;  \n",
        "\t//texture_desc.readMode = cudaReadModeElementType;\n",
        "\n",
        "\t// Here it is where the texture is actually created\n",
        "\tcudaCreateTextureObject( & texture,\n",
        "\t\t\t\t\t\t\t & resource_desc,\n",
        "\t\t\t\t\t\t\t & texture_desc,\n",
        "\t\t\t\t\t\t\t nullptr );\n",
        "\n",
        "\tcheck_cuda_call( \" Texture_Memory_Holder:  cudaCreateTextureObject() \" );\n",
        "  } // ()\n",
        "\n",
        "  // -----------------------------------------------------------------\n",
        "  // -----------------------------------------------------------------\n",
        "};\n",
        "\n",
        "// ===================================================================\n",
        "//\n",
        "// kernel\n",
        "//\n",
        "// ===================================================================\n",
        "__global__ void test_kernel_1( Result_Type * p_results,\n",
        "\t\t\t\t\t\t\t   unsigned int width,\n",
        "\t\t\t\t\t\t\t   unsigned int height,\n",
        "\t\t\t\t\t\t\t   cudaTextureObject_t in_data_texture\n",
        "\t\t\t\t\t\t\t   ) {\n",
        "\n",
        "  unsigned int x_column = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  unsigned int y_row = (blockIdx.y * blockDim.y) + threadIdx.y;\n",
        "\n",
        "  Element_Type input_val =\n",
        "\ttex2D<Element_Type>( in_data_texture, x_column+0.5f, y_row+0.5f );\n",
        "\n",
        "  p_results[ (width * y_row) + x_column ] = -input_val;\n",
        "\t\n",
        "} // ()\n",
        "\n",
        "// ===================================================================\n",
        "// ===================================================================\n",
        "template<typename T>\n",
        "auto make_up_some_data(\n",
        "\t\t\t\t\t   unsigned int NUM_ROWS,\n",
        "\t\t\t\t\t   unsigned int NUM_COLUMNS\n",
        "\t\t\t\t\t   ) {\n",
        "  \n",
        "  //\n",
        "  // Malloc on the host\n",
        "  //\n",
        "  T (*p_data)[NUM_COLUMNS] =\n",
        "\t(T (*)[NUM_COLUMNS]) my_malloc_2D<T>( NUM_ROWS, NUM_COLUMNS );\n",
        "  // The casting to T (*)[NUM_COLUMNS] is for\n",
        "  // using 2D indexing (i.e [i][j]) instead\n",
        "  // of doing the maths [ i*num_cols + j ] ourselves.\n",
        "\n",
        "  printf( \" got the memory for input data \\n\" );\n",
        "\n",
        "  //\n",
        "  // Fill in the data\n",
        "  // Each element is a float: row.col. Ex. 10.15 is row 10, col 15\n",
        "  //\n",
        "  for ( unsigned int row = 0; row < NUM_ROWS; row++ ) {\n",
        "\t//printf( \" row %d \\n\", row );\n",
        "\tfor ( unsigned int col = 0; col < NUM_COLUMNS; col++ ) {\n",
        "\t  p_data[ row ][ col ] = row + col/1000.0;\n",
        "\t} // for\n",
        "  } // for\n",
        "\n",
        "  //printf( \" %f \\n\", p_data2[ 10*WIDTH_COLUMNS_X + 15 ] );\n",
        "  printf( \" %f \\n\", p_data[10][15] );\n",
        "\n",
        "  \n",
        "  //\n",
        "  //\n",
        "  //\n",
        "  return (T (*)[]) p_data;\n",
        "\n",
        "} // ()\n",
        "\n",
        "// ===================================================================\n",
        "//\n",
        "// main\n",
        "//\n",
        "// ===================================================================\n",
        "int main( int n_args, char * args[] ) {\n",
        "\n",
        "  printf( \" starting \\n\" );\n",
        "\n",
        "  // .................................................................\n",
        "  // Create the input data for the kernels\n",
        "  // to compute something on it\n",
        "  // .................................................................\n",
        "  auto p_data =\n",
        "\tmake_up_some_data<Element_Type>( HEIGHT_ROWS_Y, WIDTH_COLUMNS_X );\n",
        "  \n",
        "  printf( \" input data generated \\n\" );\n",
        " \n",
        "  // .................................................................\n",
        "  // Copy the input data to the device, in a texture\n",
        "  // .................................................................\n",
        "  Texture_Memory_Holder<Element_Type>\n",
        "\tdata_in_texture( p_data, HEIGHT_ROWS_Y, WIDTH_COLUMNS_X ); \n",
        "\n",
        "  printf( \" placed input data in device memory, bound to a texture \\n\" );\n",
        "\t\t\t\t\t   \n",
        "  // .................................................................\n",
        "  // Get memory to hold the results (on the GPU and on the CPU)\n",
        "  // Let's suppose that we get a result for each input element.\n",
        "  // .................................................................\n",
        "  Results_Holder<Result_Type>\n",
        "\tresults( HEIGHT_ROWS_Y, WIDTH_COLUMNS_X );\n",
        "\n",
        "  printf( \" got data for the results \\n\" );\n",
        "\t\t\t\t   \n",
        "  // .................................................................\n",
        "  // set up the launch of kernels\n",
        "  // .................................................................\n",
        "  // Number of blocks we need considering a thread per element (pixel)\n",
        "  // in the 2D data\n",
        "  // Defined in 2D.\n",
        "  //\n",
        "  dim3 NUM_BLOCKS( WIDTH_COLUMNS_X / THREADS_PER_BLOCK.x,\n",
        "\t\t\t\t   HEIGHT_ROWS_Y / THREADS_PER_BLOCK.y );\n",
        "\n",
        "  // .................................................................\n",
        "  // Launch the kernel\n",
        "  // .................................................................\n",
        "  printf( \" launching kernels \\n\" );\n",
        "  test_kernel_1<<< NUM_BLOCKS, THREADS_PER_BLOCK >>>\n",
        "\t(\n",
        "\t results.results_on_device,\n",
        "\t WIDTH_COLUMNS_X,\n",
        "\t HEIGHT_ROWS_Y,\n",
        "\t data_in_texture.texture\n",
        "\t );\n",
        "  \n",
        "\n",
        "  // .................................................................\n",
        "  // wait\n",
        "  // .................................................................\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  check_cuda_call( \" kernels done\\n\" );\n",
        "  \n",
        "  printf( \" kernels done \\n\" );\n",
        "\n",
        "  // .................................................................\n",
        "  // Copy results from memory device\n",
        "  // .................................................................\n",
        "  results.copy_results_device_to_host();\n",
        "\n",
        "  // show sth. to check if the kernel has done something\n",
        "  printf( \" %f \\n\", results(10, 15) );\n",
        "  printf( \" %f \\n\", results(25, 40) );\n",
        "  printf( \" %f \\n\", results(5, 5) );\n",
        "\n",
        "  //printf( \" %f \\n\", results.results_on_host[10][15] );\n",
        "\n",
        "  // .................................................................\n",
        "  // free the memory\n",
        "  // .................................................................\n",
        "  // Memory on the host (CPU)\n",
        "  cudaFree( p_data );\n",
        "  \n",
        "  // The memory for the results ( host and device ) is freed by\n",
        "  // the destructor of results\n",
        "\n",
        "  // The memory on the texture is freed by\n",
        "  // the destructor of data_texture\n",
        "\n",
        "  // .................................................................\n",
        "  // .................................................................\n",
        "  printf( \" all done \\n\" );\n",
        "} // ()\n",
        "\n",
        "// ===================================================================\n",
        "// ===================================================================\n",
        "// ===================================================================\n",
        "// ===================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04JbWl-eFwrI",
        "outputId": "f2a28049-e066-4a2f-e97c-a4d107a12801"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " starting \n",
            " got the memory for input data \n",
            " 10.015000 \n",
            " input data generated \n",
            " Texture_Memory_Holder: element_type_size=4, rows=128, cols=128, total_size=65536\n",
            " placed input data in device memory, bound to a texture \n",
            " got data for the results \n",
            " launching kernels \n",
            " kernels done \n",
            " -10.015000 \n",
            " -25.040001 \n",
            " -5.005000 \n",
            " all done \n",
            " results memory (host and device) freed \n",
            " data_on_device and texture memory freed \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZVB6eyQRrs3",
        "outputId": "7c0395cf-1fa1-430b-c07b-351d9bdb25b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-q9u_lr7e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-q9u_lr7e\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4304 sha256=3f49f89767c856de0ee39831ce25691a68193637214915b789dabc1ba0a11111\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-axbyw1p2/wheels/f3/08/cc/e2b5b0e1c92df07dbb50a6f024a68ce090f5e7b2316b41756d\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oim5Yap2R2B6",
        "outputId": "02186f4c-510c-45b3-d62b-1b035e99830b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdint.h>\n",
        "\n",
        "typedef uint8_t mt;  // use an integer type\n",
        "\n",
        "__global__ void kernel(cudaTextureObject_t tex)\n",
        "{\n",
        "  int x = threadIdx.x;\n",
        "  int y = threadIdx.y;\n",
        "  mt val = tex2D<mt>(tex, x, y);\n",
        "  printf(\"%d, \", val);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  cudaDeviceProp prop;\n",
        "  cudaGetDeviceProperties(&prop, 0);\n",
        "  printf(\"texturePitchAlignment: %lu\\n\", prop.texturePitchAlignment);\n",
        "  cudaTextureObject_t tex;\n",
        "  const int num_rows = 4;\n",
        "  const int num_cols = prop.texturePitchAlignment*2; // should be able to use a different multiplier here\n",
        "  const int ts = num_cols*num_rows;\n",
        "  const int ds = ts*sizeof(mt);\n",
        "  mt dataIn[ds];\n",
        "  for (int i = 0; i < ts; i++) dataIn[i] = i;\n",
        "  mt* dataDev = 0;\n",
        "  cudaMalloc((void**)&dataDev, ds);\n",
        "  cudaMemcpy(dataDev, dataIn, ds, cudaMemcpyHostToDevice);\n",
        "  struct cudaResourceDesc resDesc;\n",
        "  memset(&resDesc, 0, sizeof(resDesc));\n",
        "  resDesc.resType = cudaResourceTypePitch2D;\n",
        "  resDesc.res.pitch2D.devPtr = dataDev;\n",
        "  resDesc.res.pitch2D.width = num_cols;\n",
        "  resDesc.res.pitch2D.height = num_rows;\n",
        "  resDesc.res.pitch2D.desc = cudaCreateChannelDesc<mt>();\n",
        "  resDesc.res.pitch2D.pitchInBytes = num_cols*sizeof(mt);\n",
        "  struct cudaTextureDesc texDesc;\n",
        "  memset(&texDesc, 0, sizeof(texDesc));\n",
        "  cudaCreateTextureObject(&tex, &resDesc, &texDesc, NULL);\n",
        "  dim3 threads(4, 4);\n",
        "  kernel<<<1, threads>>>(tex);\n",
        "  cudaDeviceSynchronize();\n",
        "  printf(\"\\n\");\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "8T73XJnamuO7",
        "outputId": "d4b438bb-7dee-4c9c-f522-4b93b4cafc43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texturePitchAlignment: 32\n",
            "0, 1, 2, 3, 64, 65, 66, 67, 128, 129, 130, 131, 192, 193, 194, 195, \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}