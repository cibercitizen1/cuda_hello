{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cibercitizen1/cuda_hello/blob/main/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liK2l-IVWIcR",
        "outputId": "0c932764-9f44-4b14-908f-7059dffee038"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxYOiK5mWOAh",
        "outputId": "c7eebc83-ba12-49f0-bae5-9ac4b4748f23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-yqfsoygc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-yqfsoygc\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4304 sha256=1b90098fec2f6d41af785690ba3605a4d10f6dbb7fe9eff0497c1b08a3da7a02\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xkcky86k/wheels/f3/08/cc/e2b5b0e1c92df07dbb50a6f024a68ce090f5e7b2316b41756d\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEV5vdolWTXh",
        "outputId": "994e70c3-e4e6-450d-ed9b-183cdf7d69fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "// -------------------------------------------------------------\n",
        "// mainHello_1.cu\n",
        "// -------------------------------------------------------------\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        "\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "using namespace std;\n",
        "\n",
        "// -------------------------------------------------------------\n",
        "// Z -> () -> Z (for a kernel)\n",
        "//\n",
        "// [Z] -> () -> [Z] (for all the kernels)\n",
        "// -------------------------------------------------------------\n",
        "__global__ void test_kernel(int* p_input, int* p_output) {\n",
        "\n",
        "  //\n",
        "  // We wave 1-dim data (i.e. an array)\n",
        "  // We have arranged one thread for one cell\n",
        "  // both in the input array and in the output one\n",
        "  // \n",
        "  // Therefore, we have to find out our thread index,\n",
        "  // which equates to the cell number in the array\n",
        "  // we have to manipulate\n",
        "  //\n",
        "  // The calculation is as follows:\n",
        "  \n",
        "  int idx = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "\n",
        "  // blockDim.x: the number of threads in the block for the x index\n",
        "  // (which in this case is the only one)\n",
        "  // times\n",
        "  // blockIdx.x: number of block in for the x index\n",
        "  // plus\n",
        "  // threadIdx.x: the thread number within this block\n",
        "  // Example: if we are the block number 3, each block has 16 threds\n",
        "  // and the thread number is 7\n",
        "  // The cell would be 3*16 + 7\n",
        "\n",
        "  //\n",
        "  // This is the calculation\n",
        "  //\n",
        "  p_output[idx] =  100 + p_input[idx];\n",
        "\n",
        "}\n",
        "\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "int main() {\n",
        "\n",
        "  //\n",
        "  // input and output local arrays\n",
        "  //\n",
        "  const int N=1024;\n",
        "  int numbers[N];\n",
        "  int results[N];\n",
        "\n",
        "\t int tam = N * sizeof(int);\n",
        "\n",
        "  for (int i = 0; i <= N-1; i++) {\n",
        "\tnumbers[i] = i;\n",
        "\tresults[i] = -1;\n",
        "  }\n",
        "\n",
        " \n",
        " \n",
        "\n",
        "  //\n",
        "  // get memory in the device\n",
        "  //\n",
        "  int* p_in;\n",
        "  int* p_out;\n",
        "  //\n",
        "  cudaMalloc(&p_in, tam);\n",
        "  cudaMalloc(&p_out, tam);\n",
        "\n",
        "  //\n",
        "  // timers, define and start to count\n",
        "  //\n",
        "  cudaEvent_t start; \n",
        "  cudaEvent_t end;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&end);\n",
        "  \n",
        "  cudaEventRecord(start);\n",
        "\n",
        "  //\n",
        "  // copy to device\n",
        "  //\n",
        "  cudaMemcpy(p_in, numbers, tam, cudaMemcpyHostToDevice);\n",
        "\n",
        "  dim3 total_blocks( 4 );\n",
        "  dim3 threads_per_block( N/4 );\n",
        "  \n",
        "  // dim3 total_blocks( 1 );\n",
        "  // dim3 threads_per_block( N );\n",
        "\n",
        "\n",
        "  //\n",
        "  // start up the kernel(s)\n",
        "  //\n",
        "  test_kernel<<<total_blocks, threads_per_block>>>(p_in, p_out);\n",
        "\n",
        "  //\n",
        "  // wait for completion\n",
        "  //\n",
        "  cudaEventSynchronize(end);\n",
        "\n",
        "  //\n",
        "  // copy from device\n",
        "  //\n",
        "  cudaMemcpy(&results[0], p_out,  tam, cudaMemcpyDeviceToHost);\n",
        "  \n",
        "  //\n",
        "  // record end moment, and calculate the elapsed time\n",
        "  //\n",
        "  cudaEventRecord(end);\n",
        "  float time = 0;\n",
        "  cudaEventElapsedTime(&time, start, end);\n",
        "\n",
        "  //\n",
        "  // results\n",
        "  //\n",
        "\n",
        "  cout << \"results[1] : \" << results[1] << endl;\n",
        "\n",
        "  cout << \"results[31] : \" << results[31] << endl;\n",
        "  cout << \"results[32] : \" << results[32] << endl;\n",
        "\n",
        "  cout << \"results[63] : \" << results[63] << endl;\n",
        "  cout << \"results[64] : \" << results[64] << endl;\n",
        "  cout << \"results[65] : \" << results[65] << endl;\n",
        "  cout << \"results[\" << N-1 << \"] : \" << results[N-1] << endl;\n",
        "\n",
        "\n",
        "  cout << \"start: \" << start << endl;\n",
        "  cout << \"end: \" << end << endl;\n",
        "  //cout << (end - start) << endl;\n",
        "  cout << \"The time required : \";\n",
        "  cout << time << endl;\n",
        "} // main()\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------\n",
        "// -------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v_OM1eJehzs",
        "outputId": "9804a64a-567c-4cec-c46f-b070987bb5da"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results[1] : 101\n",
            "results[31] : 131\n",
            "results[32] : 132\n",
            "results[63] : 163\n",
            "results[64] : 164\n",
            "results[65] : 165\n",
            "results[1023] : 1123\n",
            "start: 0x561ee12456c0\n",
            "end: 0x561ee1245860\n",
            "The time required : 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqYHXApU9Us7",
        "outputId": "8f491624-97db-4cfd-dd74-a80168f88a9c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oths9WePyIr5",
        "outputId": "3c4198bc-3a6d-4aea-b45e-4c45e3dfd665"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "/* This sample queries the properties of the CUDA devices present in the system via CUDA Runtime API. */\n",
        "\n",
        "// includes, system\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "\n",
        "// includes, project\n",
        "// #include <cutil.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "int\n",
        "main( int argc, char** argv) \n",
        "{\n",
        "    printf(\"CUDA Device Query (Runtime API) version (CUDART static linking)\\n\");\n",
        "\n",
        "    int deviceCount = 0;\n",
        "\n",
        "\tif (cudaGetDeviceCount(&deviceCount) != cudaSuccess) {\n",
        "\t\tprintf(\"cudaGetDeviceCount failed! CUDA Driver and Runtime version may be mismatched.\\n\");\n",
        "\t\tprintf(\"\\nTest FAILED!\\n\");\n",
        "\t\treturn 0;\n",
        "\t}\n",
        "\n",
        "    // This function call returns 0 if there are no CUDA capable devices.\n",
        "    if (deviceCount == 0)\n",
        "        printf(\"There is no device supporting CUDA\\n\");\n",
        "\n",
        "    int dev;\n",
        "    for (dev = 0; dev < deviceCount; ++dev) {\n",
        "        cudaDeviceProp deviceProp;\n",
        "        cudaGetDeviceProperties(&deviceProp, dev);\n",
        "\n",
        "        if (dev == 0) {\n",
        "\t\t\t// This function call returns 9999 for both major & minor fields, if no CUDA capable devices are present\n",
        "            if (deviceProp.major == 9999 && deviceProp.minor == 9999)\n",
        "                printf(\"There is no device supporting CUDA.\\n\");\n",
        "            else if (deviceCount == 1)\n",
        "                printf(\"There is 1 device supporting CUDA\\n\");\n",
        "            else\n",
        "                printf(\"There are %d devices supporting CUDA\\n\", deviceCount);\n",
        "        }\n",
        "        printf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
        "    #if CUDART_VERSION >= 2020\n",
        "\t\tint driverVersion = 0, runtimeVersion = 0;\n",
        "\t\tcudaDriverGetVersion(&driverVersion);\n",
        "\t\tprintf(\"  CUDA Driver Version:                           %d.%d\\n\", driverVersion/1000, driverVersion%100);\n",
        "\t\tcudaRuntimeGetVersion(&runtimeVersion);\n",
        "\t\tprintf(\"  CUDA Runtime Version:                          %d.%d\\n\", runtimeVersion/1000, runtimeVersion%100);\n",
        "    #endif\n",
        "\n",
        "        printf(\"  CUDA Capability Major revision number:         %d\\n\", deviceProp.major);\n",
        "        printf(\"  CUDA Capability Minor revision number:         %d\\n\", deviceProp.minor);\n",
        "\n",
        "\t\tprintf(\"  Total amount of global memory:                 %u bytes\\n\", deviceProp.totalGlobalMem);\n",
        "    #if CUDART_VERSION >= 2000\n",
        "        printf(\"  Number of multiprocessors:                     %d\\n\", deviceProp.multiProcessorCount);\n",
        "        printf(\"  Number of cores:                               %d\\n\", 8 * deviceProp.multiProcessorCount);\n",
        "    #endif\n",
        "        printf(\"  Total amount of constant memory:               %u bytes\\n\", deviceProp.totalConstMem); \n",
        "        printf(\"  Total amount of shared memory per block:       %u bytes\\n\", deviceProp.sharedMemPerBlock);\n",
        "        printf(\"  Total number of registers available per block: %d\\n\", deviceProp.regsPerBlock);\n",
        "        printf(\"  Warp size:                                     %d\\n\", deviceProp.warpSize);\n",
        "        printf(\"  Maximum number of threads per block:           %d\\n\", deviceProp.maxThreadsPerBlock);\n",
        "        printf(\"  Maximum sizes of each dimension of a block:    %d x %d x %d\\n\",\n",
        "               deviceProp.maxThreadsDim[0],\n",
        "               deviceProp.maxThreadsDim[1],\n",
        "               deviceProp.maxThreadsDim[2]);\n",
        "        printf(\"  Maximum sizes of each dimension of a grid:     %d x %d x %d\\n\",\n",
        "               deviceProp.maxGridSize[0],\n",
        "               deviceProp.maxGridSize[1],\n",
        "               deviceProp.maxGridSize[2]);\n",
        "        printf(\"  Maximum memory pitch:                          %u bytes\\n\", deviceProp.memPitch);\n",
        "        printf(\"  Texture alignment:                             %u bytes\\n\", deviceProp.textureAlignment);\n",
        "        printf(\"  Clock rate:                                    %.2f GHz\\n\", deviceProp.clockRate * 1e-6f);\n",
        "    #if CUDART_VERSION >= 2000\n",
        "        printf(\"  Concurrent copy and execution:                 %s\\n\", deviceProp.deviceOverlap ? \"Yes\" : \"No\");\n",
        "    #endif\n",
        "    #if CUDART_VERSION >= 2020\n",
        "        printf(\"  Run time limit on kernels:                     %s\\n\", deviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n",
        "        printf(\"  Integrated:                                    %s\\n\", deviceProp.integrated ? \"Yes\" : \"No\");\n",
        "        printf(\"  Support host page-locked memory mapping:       %s\\n\", deviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n",
        "        printf(\"  Compute mode:                                  %s\\n\", deviceProp.computeMode == cudaComputeModeDefault ?\n",
        "\t\t\t                                                            \"Default (multiple host threads can use this device simultaneously)\" :\n",
        "\t\t                                                                deviceProp.computeMode == cudaComputeModeExclusive ?\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Exclusive (only one host thread at a time can use this device)\" :\n",
        "\t\t                                                                deviceProp.computeMode == cudaComputeModeProhibited ?\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Prohibited (no host thread can use this device)\" :\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Unknown\");\n",
        "    #endif\n",
        "\t}\n",
        "    printf(\"\\nTest PASSED\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "FAvTzcmD854H",
        "outputId": "fdfa274d-ec14-4c96-8f6a-f3406e00821d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "There is 1 device supporting CUDA\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version:                           11.60\n",
            "  CUDA Runtime Version:                          11.20\n",
            "  CUDA Capability Major revision number:         7\n",
            "  CUDA Capability Minor revision number:         5\n",
            "  Total amount of global memory:                 2958950400 bytes\n",
            "  Number of multiprocessors:                     40\n",
            "  Number of cores:                               320\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per block:           1024\n",
            "  Maximum sizes of each dimension of a block:    1024 x 1024 x 64\n",
            "  Maximum sizes of each dimension of a grid:     2147483647 x 65535 x 65535\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Clock rate:                                    1.59 GHz\n",
            "  Concurrent copy and execution:                 Yes\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated:                                    No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Compute mode:                                  Default (multiple host threads can use this device simultaneously)\n",
            "\n",
            "Test PASSED\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}